# Docker Tutorial

## What is a container?
A container is a sandboxed process on your machine that is isolated from all other processes
on the host machine. That isolation leverages kernel namespaces and cgroups, features that haven been in Linux for a long time. Docker has worked to make these capabilities approachable and easy to use. 

Taken from [Discussion on containers](https://medium.com/@saschagrunert/demystifying-containers-part-i-kernel-space-2c53d6979504)
Containers have to fulfill four major requirements to be acceptable as such:
* Not negotiable: They have to run on a single host. 
* Clearly: They are groups of processes. You might know that Linux processes live inside a tree structure, 
so we can say containers must have a root process.
* Okay: they need to be isolated, whatever this means in detail.
* Not so clear: They have to fulfill common features. Features in general seem to change over time, so we have to point out what the most common features are.

## chroot
Mostly every UNIX OS has the possibility to change the root directory of the current running process (and its children). This originates from the first occurrence of chroot in UNIX version 7, from where it continued the journey into the awesome Berkeley Software Distribution (BSD). In Linux you can nowadays [chroot](https://man7.org/linux/man-pages/man2/chroot.2.html) as system call or the corresponding standalone wrapper program. 

Chroot is also referenced as "jail", because some person used it as a honeypot to monitor a security hacker back in 1991. 

```shell
mkdir -p new-root/{bin,lib64}
cp /bin/bash new-root/bin
cp /lib64/{ld-linux-x86-64.so*, libc.so*, libdl.sp.2, liberadline.so*,libtinfo.so*} new-root/lib64
sudo chroot new-root
```
We pretty much done this:
![Example](./imgCont.png)

We create a new root directory, copy a bash shell and its dependencies in and run chroot.


## What is a container image?
When running a container, it uses an isolated filesystem. This custom filesystem is provided by a container image. Since the image contains the container's filesystem, it must contain everything needed to run an application - all dependencies, scripts, binares, etc. The image also contains other configuration for the container, such as environment variables, a default command to run, and other metadata.

## Useful commands
* FROM:  instruction initializes a new build stage and sets the Base Image for subsequent instructions. As such, a valid Dockerfile must start with a **FROM** instruction. The image can be any valid image -it is especially easy to start by **pulling an image** from the Public Repositories.
- **ARG** is the only instruction that may precede **FROM** in the **DockerFile**. 
- **FROM** can appear multiple times within a single **Dockerfile** to create multiple images or use one build stage as a dependency for another. Simply make a note of the last image ID output by the commit before each new **FROM** instruction. Each **FROM** instruction clears any state created by previous instructions.
- Optionally a name can be given to a new build stage by adding **AS name** to the **FROM** instruction. The name can be used in subsequent **FROM** and **COPY --from=<name>** instructions to refer to the image built in this stage.
- The **tag** or **digest** values are optional. If you imit either of them, the builder assumes a latest tag by default. The builder returns an error if it cannot find the **tag** value.
``` dockerfile
ARG CODE_VERSION=latest
FROM base:${CODE_VERSION}
CMD /code/run-app

FROM extras:${CODE_VERSION}
CMD /code/run-extras


```
* WORKDIR
``` dockerfile
WORKDIR /path/to/workdir
```
The WORKDIR instruction sets the working directory for any RUN, CMD, ENTRYPOINT, COPY and ADD instructions that follow it in the Dockerfile. If the WORKDIR doesn't exist, it will be created even if itÿs not used in any subsequent Dockerfile instruction.
i
The WORKDIR instruction can be used multiple times in a Dockerfile. If a relative path is provided, it will be relative to the path of the previous WORKDIR instruction. For example:
* COPY
* RUN
* CMD
* EXPOSE

## Volumes
Provide the ability to connect specific filesystem paths of the container back to the host machine. If you mount a directory in the container, changes in that directory are also seen on the host machine. If you mount that same directory across container restarts, youÿd see the same files.

Volumes are the preferred mechanism for persisting data generated by and used by Docker containers. While bind mounts are dependent on the directory structure and OS of the host machine, volumes are completely managed by Docker. Volumes have several advantages over bind mounts:
* Volumes are easier to back up or migrate that bind mounts.
* You can manage volumes using Docker CLI commands or the docker API. 
* Volumes work on both Linux and Windows containers.
* Volumes can be more safely shared among multiple containers.
* Volume drivers let you store volumes on remote hosts or cloud providers, to encrypt the contents of volumes, or to add other functionality.
* New volumes can have their content pre-populated by a container.
* Volumes on Docker desktop have much higher performance than bind mounts from Mac and Windows hosts.
![Ex](https://docs.docker.com/storage/images/types-of-mounts-volume.png)


## Bind mounts
A bind mount is another type of mount, which lets you share a directory from the host's filesystem into the container. When working on an application, you can use a bind mount source code into the container. The container sees the changes you make to the code immediately, as soon as you save a file. This means that you can run processes in the container that watch for filesystem changes and respond to them.

|                                              | Named volumes                                    | Bind mounts                                        |
|----------------------------------------------|--------------------------------------------------|----------------------------------------------------|
| Host location                                | Docker chooses                                   | You decide                                         |
| Mount example (using  --mount)               | type=volume,src=my-volume,target=/usr/local/data | type=bind,src=/path/to/data,target=/usr/local/data |
| Populates new volume with container contents | Yes                                              | No                                                 |
| Supports Volume Drivers                      | Yes                                              | No                                                 |


## Example container
``` console
docker run -dp 3000:3000 `-w /app --mount "type=bind,src=$pwd,target=/app" `
node:18-alpine `
sh -c "yarn install && yarn run dev"
```

* -dp 3000:3000 : Run in detached (background) mode and create a port mapping.
* -w /app : sets the "working directory" or the current directory that the command will runfrom
* --mount type=bind,src="$(pwd)",target=/app : bind mount the current directory from the host into the /app container.
* node:18-alpine : the image to use. Note that this is the base image for our app from the Dockerfile.
* sh -c "yarn install && yarn run dev" : the command. We're starting a shell using sh (alpine doesn't have bash) and running yarn install packages and then running yarn run dev to start the development server. If we look in the package.json, we'll see that the dev script starts nodemon.

## Multi container apps

Remember that containers, by default, run in isolation and don't know anything about other processes or containers on the same machine. So, how do you allow one container to talk to another? The answer is networking. If you place the two containers on the same network, they can talk to each other.

There are two ways to put a container on a network:

* Assign the network when starting the container.
* Connect an already running container to a network.

``` shell
docker network create todo-app
```
